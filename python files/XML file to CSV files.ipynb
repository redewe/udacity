{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Convert Data from XML to CSV\n",
    "\n",
    "\n",
    "Here is the full length of code depicting how I've converted the files to CSV with help from the code in the Case Study\n",
    "https://classroom.udacity.com/nanodegrees/nd002/parts/860b269a-d0b0-4f0c-8f3d-ab08865d43bf/modules/316820862075461/lessons/5436095827/concepts/54908788190923\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Declare all libraries\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "#File path declaration and open\n",
    "OSM_PATH = \"../data_input_output/singapore.osm\"\n",
    "\n",
    "#Declare regex patterns\n",
    "lower = re.compile(r'^([a-z]|_)*$') #tags that contain only lowercase letters \n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_|[0-9])*$') #tags that are with lower case and has one colon\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') #tags with problematic characters\n",
    "\n",
    "#Custom regex patterns\n",
    "custom_regex = [\n",
    "    re.compile(r'^([a-z]|_)*:([a-z]|_)*:([a-z]|_)*$'),#tags that are lower case with two colons\n",
    "    re.compile(r'^([a-z]|_)*:([a-z]|_)*:([a-z]|_|[0-9])*:([a-z]|_)*$'),  #tags that are lower case with three colons\n",
    "    re.compile(r'^(W|T)[0-9].*([A-Z]|[0-9])$'), #tags that locate buildings within Singapore's Polytechnic building\n",
    "    re.compile(r'^ISO[0-9]'), #ISO tags\n",
    "    re.compile(r'^(currency:|Update_Sta|3dr:|catmp|name:|Max_HDOP|Max_PDOP|GPS_|GNSS_|Easting|Northing|Latitude|Longitude|Feat_Name|Filt_Pos|Unfilt_Pos|Corr_Type|Rcvr_Type|Vert_Prec|Std_Dev|Point_ID|Comment|Horz_Prec|OBJECTID)')\n",
    "    #roadID tags unique for this particular OSM / tags that capture different names of singapore / GPS/GNSS Tags, UTM and other Geo values\n",
    "]\n",
    "\n",
    "#helper function to loop through custom regex\n",
    "def regex_check(k_attr, keys):\n",
    "    for index, item in enumerate(custom_regex):\n",
    "        if item.match(k_attr):\n",
    "            keys[\"custom_regex\"] = key_value(\"custom_regex\", keys)\n",
    "            return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Analysis on structure\n",
    "\n",
    "This part of the code attempts to do a simple analysis on some of the data errors before exporting to SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users \n",
      "++++++++++++++++++++++++\n",
      "2187\n",
      "\n",
      "\n",
      "Number of tags per tag type \n",
      "++++++++++++++++++++++++\n",
      "[('nd', 1887761),\n",
      " ('node', 1517456),\n",
      " ('tag', 793406),\n",
      " ('way', 237891),\n",
      " ('member', 119328),\n",
      " ('relation', 2854),\n",
      " ('bounds', 1),\n",
      " ('osm', 1)]\n",
      "\n",
      "\n",
      "Number of tag key types and other k values  \n",
      "++++++++++++++++++++++++\n",
      "[('lower', 608220),\n",
      " ('lower_colon', 172057),\n",
      " ('custom_regex', 12881),\n",
      " ('other', 248)]\n",
      "\n",
      "\n",
      "Other k values (Unique: 48 ) \n",
      "++++++++++++++++++++++++\n",
      "[('Datafile', 62),\n",
      " ('Data_Dicti', 62),\n",
      " ('source_1', 37),\n",
      " ('FIXME', 12),\n",
      " ('name_1', 11),\n",
      " ('name_2', 5),\n",
      " ('building_1', 5),\n",
      " ('leisure_1', 4),\n",
      " ('name_3', 3),\n",
      " ('description2', 3),\n",
      " ('landuse_1', 3),\n",
      " ('amenity_1', 2),\n",
      " ('name_4', 2),\n",
      " ('Id', 2),\n",
      " ('Jalan', 2),\n",
      " ('naptan:Bearing', 1),\n",
      " ('country_code_iso3166_1_alpha_2', 1),\n",
      " ('SMA', 1),\n",
      " ('SPI', 1),\n",
      " ('LT5A', 1),\n",
      " ('LT5B', 1),\n",
      " ('MLT1', 1),\n",
      " ('LT 7', 1),\n",
      " ('Singapore Poly', 1),\n",
      " ('TODO', 1),\n",
      " ('DateTimeS', 1),\n",
      " ('website_1', 1),\n",
      " ('race course', 1),\n",
      " ('alt_name2', 1),\n",
      " ('maxspeed_1', 1),\n",
      " ('Type', 1),\n",
      " ('tourism_1', 1),\n",
      " ('fuel:GTL_diesel', 1),\n",
      " ('Name', 1),\n",
      " ('FolderPath', 1),\n",
      " ('Shape_Area', 1),\n",
      " ('Shape_Leng', 1),\n",
      " ('SHAPE_Leng', 1),\n",
      " ('Line_ID', 1),\n",
      " ('Avg_Horz_P', 1),\n",
      " ('Avg_Vert_P', 1),\n",
      " ('Worst_Horz', 1),\n",
      " ('Worst_Vert', 1),\n",
      " ('Bandar', 1),\n",
      " ('highway_1', 1),\n",
      " ('fuel:RON_92', 1),\n",
      " ('fuel:RON_97', 1),\n",
      " ('fuel:Diesel_Euro_5', 1)]\n"
     ]
    }
   ],
   "source": [
    "osm_file = open(OSM_PATH, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "#function to match key type\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        if element.findall('[@k]'):\n",
    "            for tag in element.findall('[@k]'):\n",
    "                if tag.attrib['k']:\n",
    "                    if problemchars.match(tag.attrib['k']):\n",
    "                        keys[\"problemchars\"] = key_value(\"problemchars\", keys)\n",
    "                    elif lower_colon.match(tag.attrib['k']):\n",
    "                        keys[\"lower_colon\"] = key_value(\"lower_colon\", keys)\n",
    "                    elif lower.match(tag.attrib['k']):\n",
    "                        keys[\"lower\"] = key_value(\"lower\", keys)\n",
    "                    else:\n",
    "                        if regex_check(tag.attrib['k'], keys) is not True:\n",
    "                            keys[\"other\"] = key_value(\"other\", keys) #other tags that do not fall into the other three categories\n",
    "                            #add value to other_keys list \n",
    "                            try:\n",
    "                                other_keys[tag.attrib['k']] += 1\n",
    "                            except:\n",
    "                                other_keys[tag.attrib['k']] = 1\n",
    "                            \n",
    "    return keys    \n",
    "\n",
    "#return 1 if key does not exist in dict\n",
    "def key_value(type_value, keys):\n",
    "    try:\n",
    "        if keys[type_value] >= 1:\n",
    "            return keys[type_value] + 1\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "#Loop through tags and count number of unique tags and users and tag types\n",
    "tags_dict = {}\n",
    "unique_users = set()\n",
    "keys = {}\n",
    "other_keys = {}\n",
    "\n",
    "for event, elem in ET.iterparse(osm_file):\n",
    "    keys = key_type(elem, keys)\n",
    "\n",
    "    if elem.tag in tags_dict.keys():\n",
    "        tags_dict[elem.tag] += 1\n",
    "        if elem.findall(\"[@uid]\"):\n",
    "            unique_users.add(elem.attrib['uid'])\n",
    "        \n",
    "    else:\n",
    "        tags_dict[elem.tag] = 1\n",
    "        if elem.findall(\"[@uid]\"):\n",
    "            unique_users.add(elem.attrib['uid'])\n",
    "        \n",
    "#Print number of unique users\n",
    "print(\"Number of unique users\",\"\\n++++++++++++++++++++++++\")\n",
    "print(len(unique_users))\n",
    "        \n",
    "        \n",
    "#Print results ordered by value\n",
    "print(\"\\n\\nNumber of tags per tag type\",\"\\n++++++++++++++++++++++++\")\n",
    "s = [(k, tags_dict[k]) for k in sorted(tags_dict, key=tags_dict.get, reverse=True)]\n",
    "pprint.pprint(s)\n",
    "\n",
    "#Print possible issuees with k value\n",
    "print(\"\\n\\nNumber of tag key types and other k values \",\"\\n++++++++++++++++++++++++\")\n",
    "s = [(k, keys[k]) for k in sorted(keys, key=keys.get, reverse=True)]\n",
    "pprint.pprint(s)\n",
    "\n",
    "print(\"\\n\\nOther k values (Unique:\",len(other_keys),\")\",\"\\n++++++++++++++++++++++++\")\n",
    "s = [(k, other_keys[k]) for k in sorted(other_keys, key=other_keys.get, reverse=True)]\n",
    "pprint.pprint(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean up Street Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#Due to the unique names of streets in Singapore, the road name can be placed in the beginning or at the end so the usual Regex strings will not work\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Jalan\", \"Lorong\", \"Way\"]\n",
    "\n",
    "#loop through list to create regex string\n",
    "expected_re = \"r\\'(\"\n",
    "for index,item in enumerate(expected):\n",
    "    if index != len(expected)-1:\n",
    "        expected_re += item\n",
    "        expected_re += \"|\"\n",
    "    else:\n",
    "        expected_re += item\n",
    "    \n",
    "expected_re += \")\\'\"\n",
    "\n",
    "street_type_re = re.compile(expected_re,re.IGNORECASE)\n",
    "\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"PKWY\": \"Parkway\"\n",
    "            }\n",
    "\n",
    "#Code for auditing taken from Case Study\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\", encoding='utf8')\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "#function to update names based on mapping\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name).group()\n",
    "    name = name.replace(m,mapping[m])\n",
    "    return name\n",
    "\n",
    "pprint.pprint(dict(audit(OSM_PATH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Converting to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cerberus\n",
    "import schema\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "#Node Fields as per schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "\n",
    "folder= \"../data_input_output/\"\n",
    "NODES_PATH = folder + \"nodes.csv\"\n",
    "NODE_TAGS_PATH = folder + \"nodes_tags.csv\"\n",
    "\n",
    "#Way Fields as per schema\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "WAYS_PATH = folder + \"ways.csv\"\n",
    "WAY_NODES_PATH = folder + \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = folder + \"ways_tags.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Helper Functions                     #  \n",
    "# ================================================== #\n",
    "\n",
    "def validate_k(k):\n",
    "    if problemchars.match(k):\n",
    "        return False\n",
    "\n",
    "#Yield element if it is the right type of tag\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "#Raise ValidationError if element does not match schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "#Extend csv.DictWriter to handle Unicode input\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "def list_tags(element, tags, default_tag_type):\n",
    "    for t in element.iter(\"tag\"):\n",
    "        tags_sub = {}\n",
    "        k = t.attrib['k']\n",
    "        tags_sub['id'] = element.attrib['id']\n",
    "        tags_sub['value'] = t.attrib['v']\n",
    "          \n",
    "        if validate_k(k) is not False:\n",
    "            if lower_colon.match(k):\n",
    "                k_colon = k.split(\":\")\n",
    "                \n",
    "                for index, item in enumerate(k_colon):\n",
    "                    if index == 0:\n",
    "                        tags_sub['type'] = item\n",
    "                    elif index == 1:\n",
    "                        tags_sub['key'] = item\n",
    "                    else:\n",
    "                        tags_sub['key'] += \":\"+item\n",
    "           \n",
    "            else:\n",
    "                tags_sub['type'] = default_tag_type\n",
    "                tags_sub['key'] = t.attrib['k']\n",
    "            tags.append(tags_sub)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return tags\n",
    "\n",
    "#Clean and shape node or way XML element to Python dict\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=problemchars, default_tag_type='regular'):\n",
    "    \n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "  \n",
    "    if element.tag == 'node':\n",
    "        for n in node_attr_fields:\n",
    "            node_attribs[n] = element.attrib[n]\n",
    "\n",
    "        if len(element) > 0 and element.find(\"tag\") is not None:\n",
    "            tags = list_tags(element, tags, default_tag_type)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "\n",
    "    elif element.tag == 'way':\n",
    "        for w in way_attr_fields:\n",
    "            way_attribs[w] = element.attrib[w]\n",
    "        if len(element) > 0:\n",
    "            if element.find(\"tag\") is not None:\n",
    "                tags = list_tags(element, tags, default_tag_type)\n",
    "            if element.find(\"nd\") is not None:\n",
    "                 for index, nd in enumerate(element.iter(\"nd\")):\n",
    "                     wn = {}\n",
    "                     wn['id'] = way_attribs['id']\n",
    "                     wn['node_id'] = nd.attrib['ref']\n",
    "                     wn['position'] = index\n",
    "                     way_nodes.append(wn)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               MAIN code                            #  \n",
    "# ================================================== #    \n",
    "    \n",
    "#Iteratively process each XML element and write to csv(s)\n",
    "\n",
    "with codecs.open(NODES_PATH, 'w', encoding='utf-8') as nodes_file, \\\n",
    "     codecs.open(NODE_TAGS_PATH, 'w', encoding='utf-8') as nodes_tags_file, \\\n",
    "     codecs.open(WAYS_PATH, 'w', encoding='utf-8') as ways_file, \\\n",
    "     codecs.open(WAY_NODES_PATH, 'w', encoding='utf-8') as way_nodes_file, \\\n",
    "     codecs.open(WAY_TAGS_PATH, 'w', encoding='utf-8') as way_tags_file:\n",
    "\n",
    "    nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "    node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "    ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "    way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "    way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "    #nodes_writer.writeheader()\n",
    "    #node_tags_writer.writeheader()\n",
    "    #ways_writer.writeheader()\n",
    "    #way_nodes_writer.writeheader()\n",
    "    #way_tags_writer.writeheader()\n",
    "\n",
    "    validator = cerberus.Validator()\n",
    "\n",
    "    for element in get_element(OSM_PATH, tags=('node', 'way')):\n",
    "        el = shape_element(element)\n",
    "\n",
    "        if el:\n",
    "            validate_element(el, validator)\n",
    "\n",
    "            if element.tag == 'node':\n",
    "                nodes_writer.writerow(el['node'])\n",
    "                node_tags_writer.writerows(el['node_tags'])\n",
    "            elif element.tag == 'way':\n",
    "                ways_writer.writerow(el['way'])\n",
    "                way_nodes_writer.writerows(el['way_nodes'])\n",
    "                way_tags_writer.writerows(el['way_tags'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### After exporting to CSV, calculate file sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Names and Sizes \n",
      "++++++++++++++++++++++++\n",
      "nodes.csv                      153.33 MB\n",
      "nodes_tags.csv                 6.07 MB\n",
      "sample.osm                     41.37 MB\n",
      "singapore.db                   255.52 MB\n",
      "singapore.osm                  328.98 MB\n",
      "ways.csv                       17.63 MB\n",
      "ways_nodes.csv                 43.46 MB\n",
      "ways_tags.csv                  27.06 MB\n"
     ]
    }
   ],
   "source": [
    "#Function to display readable filesize\n",
    "def GetHumanReadable(size,precision=2):\n",
    "    suffixes=[' B',' KB',' MB',' GB',' TB']\n",
    "    suffixIndex = 0\n",
    "    while size > 1024 and suffixIndex < 4:\n",
    "        suffixIndex += 1 #increment the index of the suffix\n",
    "        size = size/1024.0 #apply the division\n",
    "    return \"%.*f%s\"%(precision,size,suffixes[suffixIndex])\n",
    "\n",
    "thePath = \"..\\data_input_output\"\n",
    "theFiles = list(os.listdir(thePath))\n",
    "\n",
    "#Calculate size for all files here. \n",
    "theDict = dict()\n",
    "for something in theFiles: \n",
    "    try:\n",
    "        new_path = thePath+\"\\\\\"+something\n",
    "        theStats = os.stat(new_path)\n",
    "        theDict[something] = theStats\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "#Display list of files and filesize\n",
    "print(\"File Names and Sizes\", \"\\n++++++++++++++++++++++++\")\n",
    "for item in theDict:\n",
    "    print(\"{:30s} {:s}\".format(item,GetHumanReadable(theDict[item].st_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
